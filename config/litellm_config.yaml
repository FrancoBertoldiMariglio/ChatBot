# LiteLLM Configuration
# This file configures the LLM routing and fallback behavior

model_list:
  # Primary model - Gemini Flash (most cost-effective)
  - model_name: customer-service
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GOOGLE_API_KEY

  # Secondary model - GPT-4o-mini (fallback)
  - model_name: customer-service
    litellm_params:
      model: gpt-4o-mini
      api_key: os.environ/OPENAI_API_KEY

  # Tertiary model - Claude Haiku (backup)
  - model_name: customer-service
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY

  # Embedding model
  - model_name: embeddings
    litellm_params:
      model: text-embedding-3-small
      api_key: os.environ/OPENAI_API_KEY

router_settings:
  # Use cost-based routing to prefer cheaper models
  routing_strategy: cost-based-routing

  # Fallback configuration
  fallbacks:
    - customer-service: [customer-service]

  # Retry configuration
  num_retries: 2
  retry_after: 1

  # Timeout settings (in seconds)
  timeout: 30
  stream_timeout: 60

  # Budget limits (optional)
  # max_budget: 100  # USD per month

litellm_settings:
  # Enable cost tracking
  success_callback: ["langfuse"]  # Optional: for observability

  # Cache settings
  cache: true
  cache_params:
    type: redis  # or "local"
    ttl: 3600

  # Rate limiting
  max_parallel_requests: 100

general_settings:
  # Logging
  log_level: INFO

  # Security
  drop_params: true  # Drop unknown parameters
